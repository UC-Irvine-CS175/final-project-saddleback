{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+Kh92HBpyMsJgP4QBuoGC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Download and Unzip Data Subset\n","If you are using your own shared drive with the tar file of images that you would like to use for training make sure to place the id number after the `gdown` command. For example if your shared url is:\n","\n","https://drive.google.com/uc?id=1kvMIz2HryYLU_eek7w7lj_-xYSKd8rEL\n","\n","your id is: 1kvMIz2HryYLU_eek7w7lj_-xYSKd8rEL\n","\n","Notice that unix shell commands are preceded with an `!` in the jupyter notebook."],"metadata":{"id":"nlHXzSjuiF8B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oXHSsv8iBPK"},"outputs":[],"source":["# Uncomment for the 4hr-hiGy Fe&Xray subset\n","# !gdown 1kvMIz2HryYLU_eek7w7lj_-xYSKd8rEL\n","\n","# Uncomment for the full dataset\n","# !gdown 1-h0SCkI2kjJlLV_PiHYYZZVkEjnDJ9qd"]},{"cell_type":"code","source":["# Unzip files using the shell command\n","!tar -xvf bps_hi_4hr.tar    # Our subset\n","# !tar -xvf full_dataset.tar  # Full dataset (~2GB)"],"metadata":{"id":"Xlw59dqqiJyV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Alternative 1 - Use your own Drive\n","## Define a subset and split, then directly download the split from the AWS bucket for the [Biological and Physical Sciences (BPS) Microscopy Benchmark Training Dataset](https://registry.opendata.aws/bps_microscopy/)"],"metadata":{"id":"edQZ4fFgu8H0"}},{"cell_type":"code","source":["from google.colab import files    # downloading/uploading from/to your personal Drive\n","import pandas as pd               # Storing the data in a dataframe\n","import io\n","import boto3\n","from botocore import UNSIGNED\n","from botocore.config import Config\n","from io import BytesIO\n","import os"],"metadata":{"id":"I7vNO07TjVjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Navigate the UI that pops up after running the cell to get your zip/tar\n","subset_from_drive = files.upload()\n","filename = next(iter(subset_from_drive))\n","\n","if filename.endswith('.tar')\n","  !tar -xvf $filename\n","elif filename.endswith('.zip')\n","  !unzip $filename"],"metadata":{"id":"W8I51VGvuR3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Alternative 2 - Create your own subset\n","- You can do this using the meta.csv provided for the full dataset"],"metadata":{"id":"8vQfLCEJIzxt"}},{"cell_type":"code","source":["!gdown 1hvk0zqmV_JGyxta99lRfxksUyhMz3rt1"],"metadata":{"id":"OLPA_Sb1Jn1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def export_subset_meta_dose_hr(\n","    dose_Gy_specifier: list,\n","    hr_post_exposure_val: list,\n","    in_csv_path_local: str,             # path includes name of file w/ extension\n","    out_dir_csv: str) -> tuple:\n","    \"\"\"\n","    This function opens a csv file that contains the filenames of the bps microscopy data from the\n","    s3 bucket saved either locally or as a file_buffer object as a pandas dataframe. The dataframe\n","    is then sliced over the attributes of interest and written to another csv file for data\n","    versioning.\n","\n","    args:\n","      dose_Gy (list): dose_Gy is a string corresponding to the dose of interest ['hi', 'med', 'low']\n","      hr_post_exposure_val (list): hr_post_exposure_val is an integer corresponding to the hour post\n","      exposure of interest [4, 24, 48]\n","      in_csv_path_local (str): a string of input original csv file\n","      out_dir_csv (str): a string of the output directory you would like to write the subset_meta file to\n","\n","    returns:\n","      Tuple[str, int]: a tuple of the output csv file path and the number of rows in the output csv\n","      file\n","    \"\"\"\n","    # Create output directory out_dir_csv if it does not exist\n","    if not os.path.exists(out_dir_csv):\n","        #if not, make one\n","        os.makedirs(out_dir_csv)\n","\n","\n","    # Load csv file into pandas DataFrame\n","    csv_data_frame = pd.read_csv(in_csv_path_local)\n","\n","    # Check that dose_Gy and hr_post_exposure_val are valid\n","\n","    #               low, med, hi\n","    # Fe dose_Gy = [0.0, 0.3, 0.82]\n","    # Xray dose_Gy = [0.0, 0.1, 1.0]\n","    if ((csv_data_frame[\"particle_type\"] == \"Fe\") & (~csv_data_frame[\"dose_Gy\"].isin([0.0, 0.3, 0.82]))).any():\n","        raise Exception(\"One or more Fe dose values are not valid\")\n","\n","    if ((csv_data_frame[\"particle_type\"] == \"X-ray\") & (~csv_data_frame[\"dose_Gy\"].isin([0.0, 0.1, 1.0]))).any():\n","        raise Exception(\"One or more X-ray dose values are not valid\")\n","\n","    if ((~csv_data_frame[\"hr_post_exposure\"].isin([4, 24, 48]))).any():     # ~ is the bitwise NOT operator. Flips the bits of the operand.\n","        raise Exception(\"One or more exposure values are not valid\")\n","\n","    # Slice DataFrame by attributes of interest\n","    Fe_dose_gy = {'low': 0.0, 'med': 0.3, 'hi': 0.82}\n","    Xray_dose_gy = {'low': 0.0, 'med': 0.1, 'hi': 1.0}\n","\n","    # Create a list of the dose_Gy values corresponding to the dose_Gy_specifier\n","    temp_list_Fe = []\n","    temp_list_Xray = []\n","\n","    for specifier in dose_Gy_specifier:\n","        temp_list_Fe.append(Fe_dose_gy[specifier])\n","        temp_list_Xray.append(Xray_dose_gy[specifier])\n","\n","    csv_data_frame = csv_data_frame[\n","        # Add every row where the hr_post_exposure value is in the argument list passed in\n","        (csv_data_frame[\"hr_post_exposure\"].isin(hr_post_exposure_val)) &\n","        (\n","            ((csv_data_frame[\"particle_type\"] == \"Fe\") & (csv_data_frame[\"dose_Gy\"].isin(temp_list_Fe))) |\n","            ((csv_data_frame[\"particle_type\"] == \"X-ray\") & (csv_data_frame[\"dose_Gy\"].isin(temp_list_Xray)))\n","        )\n","    ]\n","\n","\n","    hr_post_val_string = '_'.join(str(item) for item in hr_post_exposure_val)\n","    gy_dose_string = '_'.join(str(item) for item in dose_Gy_specifier)\n","\n","    # Write sliced DataFrame to output csv file with same name as input csv file with\n","    # _dose_hr_post_exposure.csv appended\n","    file = os.path.splitext(in_csv_path_local)[0]\n","    new_file_path = file + \"_dose_\" + gy_dose_string + \"_hr_\" + hr_post_val_string + \"_post_exposure.csv\"\n","\n","    # Construct output csv file path using out_dir_csv and the name of the input csv file\n","    # with the dose_Gy and hr_post_exposure_val appended to the name of the input csv file\n","    # for data versioning.\n","    csv_data_frame.to_csv(new_file_path)\n","\n","    # Write sliced DataFrame to output csv file with name constructed above\n","    return(new_file_path, csv_data_frame.shape[0])"],"metadata":{"id":"488G8VUnL0ZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_dir = 'subset_csv'\n","\n","# Change arguments to match your desired subset for particle type classification\n","# By default it has the entire dataset\n","subset_new_path_fname, subset_size = export_subset_meta_dose_hr(\n","    dose_Gy_specifier=['low', 'med', 'hi'],\n","    hr_post_exposure_val=[4, 24, 48],\n","    in_csv_path_local='meta.csv',\n","    out_dir_csv=output_dir)"],"metadata":{"id":"dJCIDY-gI6-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_new_path_fname"],"metadata":{"id":"u9h5M4RSNWus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_size"],"metadata":{"id":"F65fg52oNab_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Now that you have a subset, we need to create a train/test split"],"metadata":{"id":"AXK5_CRiNsPz"}},{"cell_type":"code","source":["def train_test_split_subset_meta_dose_hr(\n","        subset_meta_dose_hr_csv_path: str,\n","        test_size: float,\n","        out_dir_csv: str,\n","        random_state: int = None,\n","        stratify_col: str = None\n","        ) -> tuple:\n","    \"\"\"\n","    This function reads in a csv file containing the filenames of the bps microscopy data for\n","    a subset selected by the dose_Gy and hr_post_exposure attributes. The function then opens\n","    the file as a pandas dataframe and splits the dataframe into train and test sets using\n","    sklearn.model_selection.train_test_split. The train and test dataframes are then exported\n","    to csv files in the same directory as the input csv file.\n","\n","    args:\n","        subset_meta_dose_hr_csv_path (str): a string of the input csv file path (full path includes filename)\n","        test_size (float or int): a float between 0 and 1 corresponding to the proportion of the data\n","        that should be in the test set. If int, represents the absolute number of test samples.\n","        out_dir_csv (str): a string of the output directory you would like to write the train and test\n","        random_state (int, RandomState instance or None, optional): controls the shuffling\n","        applied to the data before applying the split. Pass an int for reproducible output\n","        across multiple function calls.\n","        stratify (array-like or None, optional): array containing the labels for stratification.\n","        Default: None.\n","    returns:\n","        Tuple[str, str]: a tuple of the output csv file paths for the train and test sets\n","    \"\"\"\n","    if not os.path.exists(out_dir_csv):\n","        os.makedirs(out_dir_csv)\n","\n","    # Load csv file into pandas DataFrame and use the train_test_split function to split the\n","    # DataFrame into train and test sets\n","    df = pd.read_csv(subset_meta_dose_hr_csv_path)\n","    train, test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[stratify_col])\n","\n","    # Rewrite index numbers for both train and test sets to conform to order in new dataframe\n","    # (otherwise, index numbers will be out of order)\n","    train.reset_index(inplace=True, drop=True)\n","    test.reset_index(inplace=True, drop=True)\n","\n","    # Write train and test DataFrames to output csv files with same name as input csv file with\n","    # _train.csv or _test.csv appended\n","    train_file_path = os.path.splitext(subset_meta_dose_hr_csv_path)[0] + \"_train.csv\"\n","    test_file_path = os.path.splitext(subset_meta_dose_hr_csv_path)[0] + \"_test.csv\"\n","\n","    train.to_csv(train_file_path)\n","    test.to_csv(test_file_path)\n","\n","    return (train_file_path, test_file_path)"],"metadata":{"id":"ANaNbaryNwgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_test_split_subset_meta_dose_hr(\n","    subset_meta_dose_hr_csv_path=subset_new_path_fname,\n","    test_size=0.2,\n","    out_dir_csv=output_dir,\n","    random_state=42,\n","    stratify_col=\"particle_type\")"],"metadata":{"id":"PU2eTZvrODF7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#TODO\n","- save tiffs locally from the aws bucket"],"metadata":{"id":"27jKIejGEcat"}},{"cell_type":"markdown","source":["# Clone repository"],"metadata":{"id":"pmRq-LqPiOTe"}},{"cell_type":"code","source":["!git clone https://github.com/UC-Irvine-CS175/final-project-saddleback"],"metadata":{"id":"2A7QUpp8iPME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Change directory into your repository and checkout to Your Working Branch if it's not main"],"metadata":{"id":"-3bEMLYcicNa"}},{"cell_type":"code","source":["%cd final-project-saddleback"],"metadata":{"id":"whFI9jHvidB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !git checkout the-working-branch"],"metadata":{"id":"ya6wptp_i72X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confirm that your working branch is up to date\n","!git log --oneline -5"],"metadata":{"id":"ZJvtwutoifWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pull if your working branch is not up to date\n","!git pull"],"metadata":{"id":"U3MLxjpQilfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For this command to work you must have requirements.txt present in the current working directory\n","!pip install -r requirements.txt"],"metadata":{"id":"bM2yQdQnjDxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For system agnostic paths, we use pyprojroot to set a root\n","import os              # os.path.join()\n","import pyprojroot      # pyprojroot.here()\n","\n","# Appends the repository's base directory to the list of module paths\n","root = pyprojroot.find_root(pyprojroot.has_dir(\".git\"))\n","import sys\n","sys.path.append(str(root))"],"metadata":{"id":"WvpVzqbSA8wM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Important - Our BPSConfig class\n","3. `data_dir`: The directory we have our raw data saved to. Since we downloaded and un-tar'd the dataset before entering our repository, we need to properly access it from `../processed`\n","4. `train_meta_fname`: The name of the csv file holding all the filenames and labels for the training dataset\n","    - You will need to change this if you created your own subset\n","5. `val_meta_fname`: The name of the csv file holding all the filenames and labels for the validation dataset\n","    - You will need to change this if you created your own subset\n","6. `save_dir`: The directory where we want to save our model weights\n","7. `batch_size`: Number of images per batch\n","8. `max_epochs`: Number of epochs for training/validation\n","9. `accelerator`: Type of device we would like to use (CPU, GPU, TPU, etc)\n","10. `devices`: The number of devices for training. Leave at one unless you know what you're doing\n","11. `num_workers`: The number of workers from your CPU that will be preparing the dataloader for the model. For free Colab tier, try 8 at first.\n","12. `seed`: An integer that determines the outputs of randomness. Makes sure that your training results are reproducible for others to check.\n","13. `img_size`: Size of an image for our custom resize function. The pretrained models used in this example will automatically resize the images if they are not resized, but we have included this for scalability.\n","14. `model`: A string that is the name of the model. It must exactly match the names in `torch.hub` [here](https://pytorch.org/vision/stable/models.html#classification)."],"metadata":{"id":"7pWqvW-4UZMS"}},{"cell_type":"code","source":["@dataclass\n","class BPSConfig:\n","    data_dir:           str = '../processed'\n","    train_meta_fname:   str = 'meta_dose_hi_hr_4_post_exposure_train.csv'\n","    val_meta_fname:     str = 'meta_dose_hi_hr_4_post_exposure_test.csv'\n","    save_dir:           str = os.path.join(root, 'models', 'pretrained')\n","    batch_size:         int = 16\n","    max_epochs:         int = 5\n","    accelerator:        str = 'auto'\n","    devices:            int = 1\n","    num_workers:        int = 8\n","    seed:               int = 42\n","    img_size:           int = 256\n","    model:              str = 'vgg11_bn'\n","\n","bps_config = BPSConfig()"],"metadata":{"id":"QQVxhGaGku5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Requirements of our pre-trained Models\n","The pre-trained model architectures we are downloading from torch.hub require some additional preprocessing for the model to accept them as inputs.\n","\n","1. Normalization\n","  - The tif file images are uint16, meaning that the dynamic range of the pixels are between 0 and 2<sup>16</sup>, but to be useful for our model and our interpretation, we need to normalize those values as a float between 0 and 1\n","2. Resize\n","  - The pretrained models we get from torch.hub will resize and crop the images for us, but it is good practice to resize the images ourselves to maintain further transfer learning with other model architectures in the future\n","3. Three channel RGB images.\n","  - We set the single channel image to a three channel image by copying the greyscale image to three channels.\n","4. Conversion to Tensors\n","  - The model expects a Tensor as input, so this function will convert the numpy arrays to Tensors.\n","5. Dataloader\n","  - The dataloader will compose these preprocessing transformations in sequence and feed them into the model in the proper batch size indicated in our BPSConfig class."],"metadata":{"id":"FzSlM-mglScA"}},{"cell_type":"code","source":["from src.dataset.augmentation import(\n","    NormalizeBPS,\n","    ResizeBPS,\n","    ToThreeChannels,\n","    ToTensor\n",")\n","from src.dataset.bps_datamodule import BPSDataModule"],"metadata":{"id":"EcoaJJAhnt5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pytorch_lightning as pl\n","\n","# Fix random seed\n","pl.seed_everything(bps_config.seed, workers=True)"],"metadata":{"id":"FoTV6l7Tn2dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Instantiate our BPSDataModule ‚åõÔ∏è"],"metadata":{"id":"_gNdUKVAbqTp"}},{"cell_type":"code","source":["bps_datamodule = BPSDataModule(train_csv_file=bps_config.train_meta_fname,\n","                               train_dir=bps_config.data_dir,\n","                               val_csv_file=bps_config.val_meta_fname,\n","                               val_dir=bps_config.data_dir,\n","                               resize_dims=(bps_config.img_size, bps_config.img_size),\n","                               batch_size=bps_config.batch_size,\n","                               num_workers=bps_config.num_workers)\n","\n","# Using BPSDataModule's setup, define the stage name ('train' or 'validate')\n","bps_datamodule.setup(stage='train')       #training set dataloader\n","bps_datamodule.setup(stage='validate')    #validation set dataloader"],"metadata":{"id":"osrE8bRBofyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Modules üêç & Load Pre-trained Model Weights üèã\n","\n","The default weights are selected here, which will always grab the best performing weights from torch.hub."],"metadata":{"id":"l68jbRmepSFT"}},{"cell_type":"code","source":["import torch\n","from torch.optim import Adam\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transform, utils\n","from torchmetrics import Accuracy\n","\n","import numpy as no\n","import random\n","\n","import wandb\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning import Trainer\n","from pytorch_lightning import LightningModule\n","from datetime import datetime"],"metadata":{"id":"eQHby5gLTR1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_model = torch.hub.load('pytorch/vision',\n","                          bps_config.model,\n","                          weights='DEFAULT')"],"metadata":{"id":"rMvC8Av0pkQi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modify the last layers for particle type classification\n","\n","The pretrained models have different architecture that match the outputs for the data they were trained on. For our example, we need to make sure they output 2 probabilities because our model should be specified for binary classification."],"metadata":{"id":"XdcLzJfKqCCp"}},{"cell_type":"code","source":["if bps_config.model in ['resnet18', 'resnet50', 'resnet101']\n","  num_features = my_model.fc.in_features\n","  my_model.fc = nn.Linear(num_features, 2)\n","  isResNet = True\n","  isVGG = False\n","  isSqueezeNet = False\n","elif bps_config.model == 'vgg11_bn'\n","  num_features = my_model.classifier[6].in_features\n","  my_model.classifier[6] = nn.Linear(num_features, 2)\n","  isVGG = True\n","  isResNet = False\n","  isSqueezeNet = False\n","elif bps_config.model == 'squeezenet1_1'\n","  final_conv = nn.Conv2d(512, 2, kernel_size=1)\n","  my_model.classifier._modules['1'] = final_conv\n","  my_model.num_classes = 2\n","  isSqueezeNet = True\n","  isResNet = False\n","  isVGG = False"],"metadata":{"id":"C0JJzWoLqGqc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Freezing layers\n","An important part of tuning the model is experimenting with the pre-trained layers. The model will still make use of the pre-trained weights in the frozen layers, but the gradients are not updated during training.\n","\n","This is not required for training, and you can experiment with unfreezing some/all layers, or going even further an adding new layers. This will not be covered here, but just know that you can comment out the following code cell if you want every layer in the model architecture to train on your dataset, instead of the final layer that outputs a Tensor of probabilities we use to generate a prediction."],"metadata":{"id":"ofbPRwkLqUD6"}},{"cell_type":"code","source":["if isResNet\n","  for name, param in my_model.named_parameters():\n","      if \"fc\" not in name:\n","          param.requires_grad_(False)  # Set requires_grad to False\n","\n","elif (isVGG or isSqueezeNet)\n","  for name, param in my_model.named_parameters():\n","      if \"classifier\" not in name:\n","          param.requires_grad_(False)  # Set requires_grad to False"],"metadata":{"id":"fmYJc9lfqqe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The BPS Classifier Class Definition ‚ö°\n","Pytorch Lightning abstracts some of the important concepts that we take for granted, for example the training loop itself. The general premise is that since there are some things that are always present, we can have that taken care of in the background and just focus on overriding parts of the process that are specific to our implementation. See [this link](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#hooks) to go deeper and read the pseudocode. You can override any of the functions to make the model do exactly what you want, when you want.\n","\n","## `__init__`\n","- Define a model, a loss function, an accuracy function, a learning rate, and how you would like to save hyperparameters.\n","\n","## `forward`\n","- The \"forward pass\" takes an input and passes it through the layers in your model. Pytorch Lightning lets us pass in pretrained models from torch.hub and utilize our dataloader pipeline without any customization needed.\n","\n","## `training_step`\n","- Loop through all our batches over the course of an epoch, saving the loss and logging it per-epoch. Pytorch Lightning abstracted away the actual loop in another function that we are not overriding, because it will often be the same across a wide variety of Machine Learning solutions.\n","\n","## `validation_step`\n","- Similar to the training_step, but in this step we need to see how accurate the model is after learning from the previous training step(s).\n","\n","## `test_step`\n","- In the BPSClassifier, we do not have access to the test data, but it is provided for future use in case it is released or if the dataset author wishes to use it here.\n","\n","## `configure_optimizers`\n","- Set an optimizer (we use Adam here) with a learning rate. You can add a lr scheduler here as well.\n"],"metadata":{"id":"hmuiHRhMrN4j"}},{"cell_type":"code","source":["class BPSClassifier(pl.LightningModule):\n","    def __init__ (self, model, n_classes=2, lr=1e-3):\n","\n","        super().__init__()\n","        self.model = model\n","\n","        self.loss_fn = CrossEntropyLoss()\n","        self.accuracy_fn = Accuracy(task='binary', num_classes=2)\n","        self.lr = lr\n","        self.save_hyperparameters\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.forward(y)\n","\n","        loss = self.loss_fn(y_hat, y)\n","        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.forward(y)\n","\n","        y_pred = torch.argmax(y_hat, dim=1)\n","        acc = self.accuracy_fn(y_pred, y)\n","\n","        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n","        self.log(\"val_accuracy\", acc, on_step=False, on_epoch=True)\n","\n","        return preds\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.forward(y)\n","\n","        loss = self.loss_fn(y_hat, y)\n","\n","        y_pred = torch.argmax(y_hat, dim=1)\n","        acc = self.accuracy_fn(y_pred, y)\n","\n","        self.log(\"test_loss\", loss)\n","        self.log(\"test_accuracy\", acc)\n","\n","    def configure_optimizers(self):\n","        optimizer = Adam(self.parameters(), lr=self.lr)\n","        return optimizer"],"metadata":{"id":"nRbrPvF7rSc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a wandb session ü™Ñ\n","- The next cell will create a link for you the copy your authorization code. Paste that into the terminal output in the cell below after you run it so that you can upload your results to Weights & Biases."],"metadata":{"id":"ZDxbQIZpv1kf"}},{"cell_type":"code","source":["!wandb login --relogin"],"metadata":{"id":"s8qgbKFBy6TD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a unique name for your run that is descriptive\n","# We set the mode to 'online' so that the weights are not saved locally (we will use Pytorch Lightning for that)\n","current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","wandb_logger = WandbLogger(project='deeplearning-eda-saddleback',\n","                           log_model=\"all\",\n","                           entity='saddleback',\n","                           name=f'{bps_config.model}_{bps_config.max_epochs}_{current_datetime}',\n","                           mode='online')"],"metadata":{"id":"n3Pl4FnVv38e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Time to train!"],"metadata":{"id":"d6AxMyrEvoa-"}},{"cell_type":"code","source":["# Pass pre-trained model to the BPSClassifier to define the forward pass\n","my_model = BPSClassifier(my_model)\n","\n","# Initialize the trainer, and include a directory where to save model weights with Pytorch Lightning\n","trainer = Trainer(logger=wandb_logger,\n","                  accelerator=bps_config.accelerator,\n","                  max_epochs=bps_config.max_epochs,\n","                  default_root_dir='path/to/save/checkpoints')\n","\n","# Training Time!\n","trainer.fit(model=my_model,\n","            train_dataloader=bps_datamodule.train_dataloader(),\n","            val_dataloder=bps_datamodule.val_dataloder())"],"metadata":{"id":"aXmsbV3xvq_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.finish()"],"metadata":{"id":"p_SC1Tbwy4PV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Weights from Training"],"metadata":{"id":"6haAryI0HpkV"}},{"cell_type":"code","source":["model = MyLightningModule.load_from_checkpoint('path/to/save/checkpoints/example.ckpt')"],"metadata":{"id":"jX53DGd2Hr0A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The world is your oyster!\n","You can use this model however you want now.\n","\n","<img src=\"https://media.giphy.com/media/MCZ39lz83o5lC/giphy.gif\" height=200></img>"],"metadata":{"id":"l_CdHv-rIAbs"}}]}